<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>72 Scrapy入门 | Python-100天从新手到大师</title>


<link rel="stylesheet" href="/python100days/book.min.89885fa0429ec73067282f26fc0c6d38cfc617a443ec4e835d47b821c52c6e92.css" integrity="sha256-iYhfoEKexzBnKC8m/AxtOM/GF6RD7E6DXUe4IcUsbpI=">


<script defer src="/python100days/search.min.60214faf2667d486f5a5c602eb23dff270f5cbd9d003a2a159b89871099dc1b4.js" integrity="sha256-YCFPryZn1Ib1pcYC6yPf8nD1y9nQA6KhWbiYcQmdwbQ="></script>



<link rel="icon" href="/python100days/favicon.png" type="image/x-icon">


<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="flex container">

    <aside class="book-menu fixed">
      <nav>
<h2 class="book-brand">
  <a href="https://ezzdoc.github.io/python100days/"><span>Python-100天从新手到大师</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" placeholder="Search" id="book-search-input" maxlength="64" readonly />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>





    
  
  
  

  <style>
  nav ul a[href$="\2fpython100days\2f docs\2f Day66-75\2f 72-Scrapy%E5%85%A5%E9%97%A8\2f "] {
      color: #004ed0;
  }
  </style>

<ul>
<li><a href="/python100days/posts/"><strong>相关文章</strong></a></li>
<li><strong>Day01-15</strong>

<ul>
<li><a href="/python100days/docs/Day01-15/01-%E5%88%9D%E8%AF%86Python/">01.初识Python</a></li>
<li><a href="/python100days/docs/Day01-15/02-%E8%AF%AD%E8%A8%80%E5%85%83%E7%B4%A0/">02.语言元素</a></li>
<li><a href="/python100days/docs/Day01-15/03-%E5%88%86%E6%94%AF%E7%BB%93%E6%9E%84/">03.分支结构</a></li>
<li><a href="/python100days/docs/Day01-15/04-%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84/">04.循环结构</a></li>
<li><a href="/python100days/docs/Day01-15/05-%E6%9E%84%E9%80%A0%E7%A8%8B%E5%BA%8F%E9%80%BB%E8%BE%91/">05.构造程序逻辑</a></li>
<li><a href="/python100days/docs/Day01-15/06-%E5%87%BD%E6%95%B0%E5%92%8C%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/">06.函数和模块的使用</a></li>
<li><a href="/python100days/docs/Day01-15/07-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">07.字符串和常用数据结构</a></li>
<li><a href="/python100days/docs/Day01-15/08-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/">08.面向对象编程基础</a></li>
<li><a href="/python100days/docs/Day01-15/09-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%BF%9B%E9%98%B6/">09.面向对象进阶</a></li>
<li><a href="/python100days/docs/Day01-15/10-%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2%E5%92%8C%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/">10.图形用户界面和游戏开发</a></li>
<li><a href="/python100days/docs/Day01-15/11-%E6%96%87%E4%BB%B6%E5%92%8C%E5%BC%82%E5%B8%B8/">11.文件和异常</a></li>
<li><a href="/python100days/docs/Day01-15/12-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/">12.字符串和正则表达式</a></li>
<li><a href="/python100days/docs/Day01-15/13-%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/">13.进程和线程</a></li>
<li><a href="/python100days/docs/Day01-15/14-%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%E5%92%8C%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/">14.网络编程入门和网络应用开发</a></li>
<li><a href="/python100days/docs/Day01-15/15-%E5%9B%BE%E5%83%8F%E5%92%8C%E5%8A%9E%E5%85%AC%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86/">15.图像和办公文档处理</a></li>
</ul></li>
<li><strong>Day16-20</strong>

<ul>
<li><a href="/python100days/docs/Day16-20/16-20-Python%E8%AF%AD%E8%A8%80%E8%BF%9B%E9%98%B6/">16-20.Python语言进阶</a></li>
</ul></li>
<li><strong>Day21-30</strong>

<ul>
<li><a href="/python100days/docs/Day21-30/21-30-Web%E5%89%8D%E7%AB%AF%E6%A6%82%E8%BF%B0/">21-30.Web前端概述</a></li>
</ul></li>
<li><strong>Day31-35</strong>

<ul>
<li><a href="/python100days/docs/Day31-35/31-35-%E7%8E%A9%E8%BD%ACLinux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">31-35.玩转Linux操作系统</a></li>
</ul></li>
<li><strong>Day36-40</strong>

<ul>
<li><a href="/python100days/docs/Day36-40/36-38-%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93MySQL/">36-38.关系型数据库MySQL</a></li>
<li><a href="/python100days/docs/Day36-40/39-40-NoSQL%E5%85%A5%E9%97%A8/">39-40.NoSQL入门</a></li>
</ul></li>
<li><strong>Day41-55</strong>

<ul>
<li><a href="/python100days/docs/Day41-55/41-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/">41.快速上手</a></li>
<li><a href="/python100days/docs/Day41-55/42-%E6%B7%B1%E5%85%A5%E6%A8%A1%E5%9E%8B/">42.深入模型</a></li>
<li><a href="/python100days/docs/Day41-55/43-%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%92%8CAjax%E8%AF%B7%E6%B1%82/">43.静态资源和Ajax请求</a></li>
<li><a href="/python100days/docs/Day41-55/44-%E8%A1%A8%E5%8D%95%E7%9A%84%E5%BA%94%E7%94%A8/">44.表单的应用</a></li>
<li><a href="/python100days/docs/Day41-55/45-Cookie%E5%92%8CSession/">45.Cookie和Session</a></li>
<li><a href="/python100days/docs/Day41-55/46-%E6%8A%A5%E8%A1%A8%E5%92%8C%E6%97%A5%E5%BF%97/">46.报表和日志</a></li>
<li><a href="/python100days/docs/Day41-55/47-%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E5%BA%94%E7%94%A8/">47.中间件的应用</a></li>
<li><a href="/python100days/docs/Day41-55/48-%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8/">48.前后端分离开发入门</a></li>
<li><a href="/python100days/docs/Day41-55/49-RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E5%85%A5%E9%97%A8/">49.RESTful架构和DRF入门</a></li>
<li><a href="/python100days/docs/Day41-55/50-RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E8%BF%9B%E9%98%B6/">50.RESTful架构和DRF进阶</a></li>
<li><a href="/python100days/docs/Day41-55/51-%E4%BD%BF%E7%94%A8%E7%BC%93%E5%AD%98/">51.使用缓存</a></li>
<li><a href="/python100days/docs/Day41-55/52-%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%92%8C%E5%AF%8C%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91/">52.文件上传和富文本编辑</a></li>
<li><a href="/python100days/docs/Day41-55/53-%E7%9F%AD%E4%BF%A1%E5%92%8C%E9%82%AE%E4%BB%B6/">53.短信和邮件</a></li>
<li><a href="/python100days/docs/Day41-55/54-%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/">54.异步任务和定时任务</a></li>
<li><a href="/python100days/docs/Day41-55/55-%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E5%92%8C%E9%A1%B9%E7%9B%AE%E4%B8%8A%E7%BA%BF/">55.单元测试和项目上线</a></li>
</ul></li>
<li><strong>Day56-60</strong>

<ul>
<li><a href="/python100days/docs/Day56-60/56-Flask%E5%85%A5%E9%97%A8/">56.Flask入门</a></li>
<li><a href="/python100days/docs/Day56-60/57-%E6%A8%A1%E6%9D%BF%E7%9A%84%E4%BD%BF%E7%94%A8/">57.模板的使用</a></li>
<li><a href="/python100days/docs/Day56-60/58-%E8%A1%A8%E5%8D%95%E7%9A%84%E5%A4%84%E7%90%86/">58.表单的处理</a></li>
<li><a href="/python100days/docs/Day56-60/59-%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C/">59.数据库操作</a></li>
<li><a href="/python100days/docs/Day56-60/60-%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/">60.项目实战</a></li>
</ul></li>
<li><strong>Day61-65</strong>

<ul>
<li><a href="/python100days/docs/Day61-65/61-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/">61.预备知识</a></li>
<li><a href="/python100days/docs/Day61-65/62-Tornado%E5%85%A5%E9%97%A8/">62.Tornado入门</a></li>
<li><a href="/python100days/docs/Day61-65/63-%E5%BC%82%E6%AD%A5%E5%8C%96/">63.异步化</a></li>
<li><a href="/python100days/docs/Day61-65/64-WebSocket%E7%9A%84%E5%BA%94%E7%94%A8/">64.WebSocket的应用</a></li>
<li><a href="/python100days/docs/Day61-65/65-%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/">65.项目实战</a></li>
</ul></li>
<li><strong>Day66-75</strong>

<ul>
<li><a href="/python100days/docs/Day66-75/66-%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%92%8C%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7/">66.网络爬虫和相关工具</a></li>
<li><a href="/python100days/docs/Day66-75/67-%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%92%8C%E8%A7%A3%E6%9E%90/">67.数据采集和解析</a></li>
<li><a href="/python100days/docs/Day66-75/68-%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE/">68.存储数据</a></li>
<li><a href="/python100days/docs/Day66-75/69-%E5%B9%B6%E5%8F%91%E4%B8%8B%E8%BD%BD/">69.并发下载</a></li>
<li><a href="/python100days/docs/Day66-75/70-%E8%A7%A3%E6%9E%90%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9/">70.解析动态内容</a></li>
<li><a href="/python100days/docs/Day66-75/71-%E8%A1%A8%E5%8D%95%E4%BA%A4%E4%BA%92%E5%92%8C%E9%AA%8C%E8%AF%81%E7%A0%81%E5%A4%84%E7%90%86/">71.表单交互和验证码处理</a></li>
<li><a href="/python100days/docs/Day66-75/72-Scrapy%E5%85%A5%E9%97%A8/">72.Scrapy入门</a></li>
<li><a href="/python100days/docs/Day66-75/73-Scrapy%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8/">73.Scrapy高级应用</a></li>
<li><a href="/python100days/docs/Day66-75/74-Scrapy%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0/">74.Scrapy分布式实现</a></li>
<li><a href="/python100days/docs/Day66-75/75-%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/">75.爬虫项目实战</a></li>
</ul></li>
<li><strong>Day76-90</strong>

<ul>
<li><a href="/python100days/docs/Day76-90/76-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">76.机器学习基础</a></li>
<li><a href="/python100days/docs/Day76-90/77-Pandas%E7%9A%84%E5%BA%94%E7%94%A8/">77.Pandas的应用</a></li>
<li><a href="/python100days/docs/Day76-90/78-NumPy%E5%92%8CSciPy%E7%9A%84%E5%BA%94%E7%94%A8/">78.NumPy和SciPy的应用</a></li>
<li><a href="/python100days/docs/Day76-90/79-Matplotlib%E5%92%8C%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/">79.Matplotlib和数据可视化</a></li>
<li><a href="/python100days/docs/Day76-90/80-k%E6%9C%80%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB/">80.k最近邻分类</a></li>
<li><a href="/python100days/docs/Day76-90/81-%E5%86%B3%E7%AD%96%E6%A0%91/">81.决策树</a></li>
<li><a href="/python100days/docs/Day76-90/82-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB/">82.贝叶斯分类</a></li>
<li><a href="/python100days/docs/Day76-90/83-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">83.支持向量机</a></li>
<li><a href="/python100days/docs/Day76-90/84-K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB/">84.K-均值聚类</a></li>
<li><a href="/python100days/docs/Day76-90/85-%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/">85.回归分析</a></li>
<li><a href="/python100days/docs/Day76-90/86-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%85%A5%E9%97%A8/">86.大数据分析入门</a></li>
<li><a href="/python100days/docs/Day76-90/87-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%BF%9B%E9%98%B6/">87.大数据分析进阶</a></li>
<li><a href="/python100days/docs/Day76-90/88-Tensorflow%E5%85%A5%E9%97%A8/">88.Tensorflow入门</a></li>
<li><a href="/python100days/docs/Day76-90/89-Tensorflow%E5%AE%9E%E6%88%98/">89.Tensorflow实战</a></li>
<li><a href="/python100days/docs/Day76-90/90-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98/">90.推荐系统实战</a></li>
</ul></li>
<li><strong>Day91-100</strong>

<ul>
<li><a href="/python100days/docs/Day91-100/91-%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E5%87%86%E5%A4%87/">91.团队项目开发准备</a></li>
<li><a href="/python100days/docs/Day91-100/92-%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1/">92.使用Docker部署服务</a></li>
<li><a href="/python100days/docs/Day91-100/93-MySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/">93.MySQL性能优化</a></li>
<li><a href="/python100days/docs/Day91-100/94-%E7%BD%91%E7%BB%9CAPI%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1/">94.网络API接口设计</a></li>
<li><a href="/python100days/docs/Day91-100/95-%E4%BD%BF%E7%94%A8Django%E5%BC%80%E5%8F%91%E5%95%86%E4%B8%9A%E9%A1%B9%E7%9B%AE/">95.使用Django开发商业项目</a></li>
<li><a href="/python100days/docs/Day91-100/96-%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%92%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/">96.软件测试和自动化测试</a></li>
<li><a href="/python100days/docs/Day91-100/97-%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E8%A6%81%E7%82%B9%E5%89%96%E6%9E%90/">97.电商网站技术要点剖析</a></li>
<li><a href="/python100days/docs/Day91-100/98-%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%E5%92%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/">98.项目部署上线和性能调优</a></li>
<li><a href="/python100days/docs/Day91-100/99-%E9%9D%A2%E8%AF%95%E4%B8%AD%E7%9A%84%E5%85%AC%E5%85%B1%E9%97%AE%E9%A2%98/">99.面试中的公共问题</a></li>
<li><a href="/python100days/docs/Day91-100/100-%E8%8B%B1%E8%AF%AD%E9%9D%A2%E8%AF%95/">100.英语面试</a></li>
</ul></li>
</ul>







</nav>


<script>
(function() {
  var menu = document.querySelector("aside.book-menu nav");
  addEventListener("beforeunload", function(event) {
    localStorage.setItem("menu.scrollTop", menu.scrollTop);
  });
  menu.scrollTop = localStorage.getItem("menu.scrollTop");
})();
</script>

    </aside>

    <div class="book-page">
      <header class="flex align-center justify-between book-header">
  <label for="menu-control">
    <img src="/python100days/svg/menu.svg" alt="Menu" />
  </label>
  <strong>72 Scrapy入门</strong>
</header>

      
<article class="markdown">

<h1 id="scrapy爬虫框架入门">Scrapy爬虫框架入门</h1>

<h2 id="scrapy概述">Scrapy概述</h2>

<p>Scrapy是Python开发的一个非常流行的网络爬虫框架，可以用来抓取Web站点并从页面中提取结构化的数据，被广泛的用于数据挖掘、数据监测和自动化测试等领域。下图展示了Scrapy的基本架构，其中包含了主要组件和系统的数据处理流程（图中带数字的红色箭头）。</p>

<p><img src="Day66-75/res/scrapy-architecture.png" alt="" /></p>

<h3 id="组件">组件</h3>

<ol>
<li>Scrapy引擎（Engine）：Scrapy引擎是用来控制整个系统的数据处理流程。</li>
<li>调度器（Scheduler）：调度器从Scrapy引擎接受请求并排序列入队列，并在Scrapy引擎发出请求后返还给它们。</li>
<li>下载器（Downloader）：下载器的主要职责是抓取网页并将网页内容返还给蜘蛛（Spiders）。</li>
<li>蜘蛛（Spiders）：蜘蛛是有Scrapy用户自定义的用来解析网页并抓取特定URL返回的内容的类，每个蜘蛛都能处理一个域名或一组域名，简单的说就是用来定义特定网站的抓取和解析规则。</li>
<li>条目管道（Item Pipeline）：条目管道的主要责任是负责处理有蜘蛛从网页中抽取的数据条目，它的主要任务是清理、验证和存储数据。当页面被蜘蛛解析后，将被发送到条目管道，并经过几个特定的次序处理数据。每个条目管道组件都是一个Python类，它们获取了数据条目并执行对数据条目进行处理的方法，同时还需要确定是否需要在条目管道中继续执行下一步或是直接丢弃掉不处理。条目管道通常执行的任务有：清理HTML数据、验证解析到的数据（检查条目是否包含必要的字段）、检查是不是重复数据（如果重复就丢弃）、将解析到的数据存储到数据库（关系型数据库或NoSQL数据库）中。</li>
<li>中间件（Middlewares）：中间件是介于Scrapy引擎和其他组件之间的一个钩子框架，主要是为了提供自定义的代码来拓展Scrapy的功能，包括下载器中间件和蜘蛛中间件。</li>
</ol>

<h3 id="数据处理流程">数据处理流程</h3>

<p>Scrapy的整个数据处理流程由Scrapy引擎进行控制，通常的运转流程包括以下的步骤：</p>

<ol>
<li><p>引擎询问蜘蛛需要处理哪个网站，并让蜘蛛将第一个需要处理的URL交给它。</p></li>

<li><p>引擎让调度器将需要处理的URL放在队列中。</p></li>

<li><p>引擎从调度那获取接下来进行爬取的页面。</p></li>

<li><p>调度将下一个爬取的URL返回给引擎，引擎将它通过下载中间件发送到下载器。</p></li>

<li><p>当网页被下载器下载完成以后，响应内容通过下载中间件被发送到引擎；如果下载失败了，引擎会通知调度器记录这个URL，待会再重新下载。</p></li>

<li><p>引擎收到下载器的响应并将它通过蜘蛛中间件发送到蜘蛛进行处理。</p></li>

<li><p>蜘蛛处理响应并返回爬取到的数据条目，此外还要将需要跟进的新的URL发送给引擎。</p></li>

<li><p>引擎将抓取到的数据条目送入条目管道，把新的URL发送给调度器放入队列中。</p></li>
</ol>

<p>上述操作中的2-8步会一直重复直到调度器中没有需要请求的URL，爬虫停止工作。</p>

<h2 id="安装和使用scrapy">安装和使用Scrapy</h2>

<p>可以先创建虚拟环境并在虚拟环境下使用pip安装scrapy。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Shell" data-lang="Shell"></code></pre></div>
<p>项目的目录结构如下图所示。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Shell" data-lang="Shell"><span style="color:#f92672">(</span>venv<span style="color:#f92672">)</span> $ tree
.
|____ scrapy.cfg
|____ douban
| |____ spiders
| | |____ __init__.py
| | |____ __pycache__
| |____ __init__.py
| |____ __pycache__
| |____ middlewares.py
| |____ settings.py
| |____ items.py
| |____ pipelines.py</code></pre></div>
<blockquote>
<p>说明：Windows系统的命令行提示符下有tree命令，但是Linux和MacOS的终端是没有tree命令的，可以用下面给出的命令来定义tree命令，其实是对find命令进行了定制并别名为tree。</p>

<p><code>alias tree=&quot;find . -print | sed -e 's;[^/]*/;|____;g;s;____|; |;g'&quot;</code></p>

<p>Linux系统也可以通过yum或其他的包管理工具来安装tree。</p>

<p><code>yum install tree</code></p>
</blockquote>

<p>根据刚才描述的数据处理流程，基本上需要我们做的有以下几件事情：</p>

<ol>
<li><p>在items.py文件中定义字段，这些字段用来保存数据，方便后续的操作。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#75715e"># -*- coding: utf-8 -*-</span>

<span style="color:#75715e"># Define here the models for your scraped items</span>
<span style="color:#75715e">#</span>
<span style="color:#75715e"># See documentation in:</span>
<span style="color:#75715e"># https://doc.scrapy.org/en/latest/topics/items.html</span>

<span style="color:#f92672">import</span> scrapy


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DoubanItem</span>(scrapy<span style="color:#f92672">.</span>Item):

   name <span style="color:#f92672">=</span> scrapy<span style="color:#f92672">.</span>Field()
   year <span style="color:#f92672">=</span> scrapy<span style="color:#f92672">.</span>Field()
   score <span style="color:#f92672">=</span> scrapy<span style="color:#f92672">.</span>Field()
   director <span style="color:#f92672">=</span> scrapy<span style="color:#f92672">.</span>Field()
   classification <span style="color:#f92672">=</span> scrapy<span style="color:#f92672">.</span>Field()
   actor <span style="color:#f92672">=</span> scrapy<span style="color:#f92672">.</span>Field()</code></pre></div></li>

<li><p>在spiders文件夹中编写自己的爬虫。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Shell" data-lang="Shell"><span style="color:#f92672">(</span>venv<span style="color:#f92672">)</span> $ scrapy genspider movie movie.douban.com --template<span style="color:#f92672">=</span>crawl</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#75715e"># -*- coding: utf-8 -*-</span>
<span style="color:#f92672">import</span> scrapy
<span style="color:#f92672">from</span> scrapy.selector <span style="color:#f92672">import</span> Selector
<span style="color:#f92672">from</span> scrapy.linkextractors <span style="color:#f92672">import</span> LinkExtractor
<span style="color:#f92672">from</span> scrapy.spiders <span style="color:#f92672">import</span> CrawlSpider, Rule

<span style="color:#f92672">from</span> douban.items <span style="color:#f92672">import</span> DoubanItem


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MovieSpider</span>(CrawlSpider):
   name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;movie&#39;</span>
   allowed_domains <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;movie.douban.com&#39;</span>]
   start_urls <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;https://movie.douban.com/top250&#39;</span>]
   rules <span style="color:#f92672">=</span> (
       Rule(LinkExtractor(allow<span style="color:#f92672">=</span>(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;https://movie.douban.com/top250\?start=\d+.*&#39;</span>))),
       Rule(LinkExtractor(allow<span style="color:#f92672">=</span>(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;https://movie.douban.com/subject/\d+&#39;</span>)), callback<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;parse_item&#39;</span>),
   )

   <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_item</span>(self, response):
       sel <span style="color:#f92672">=</span> Selector(response)
       item <span style="color:#f92672">=</span> DoubanItem()
       item[<span style="color:#e6db74">&#39;name&#39;</span>]<span style="color:#f92672">=</span>sel<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//*[@id=&#34;content&#34;]/h1/span[1]/text()&#39;</span>)<span style="color:#f92672">.</span>extract()
       item[<span style="color:#e6db74">&#39;year&#39;</span>]<span style="color:#f92672">=</span>sel<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//*[@id=&#34;content&#34;]/h1/span[2]/text()&#39;</span>)<span style="color:#f92672">.</span>re(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;\((\d+)\)&#39;</span>)
       item[<span style="color:#e6db74">&#39;score&#39;</span>]<span style="color:#f92672">=</span>sel<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//*[@id=&#34;interest_sectl&#34;]/div/p[1]/strong/text()&#39;</span>)<span style="color:#f92672">.</span>extract()
       item[<span style="color:#e6db74">&#39;director&#39;</span>]<span style="color:#f92672">=</span>sel<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//*[@id=&#34;info&#34;]/span[1]/a/text()&#39;</span>)<span style="color:#f92672">.</span>extract()
       item[<span style="color:#e6db74">&#39;classification&#39;</span>]<span style="color:#f92672">=</span> sel<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//span[@property=&#34;v:genre&#34;]/text()&#39;</span>)<span style="color:#f92672">.</span>extract()
       item[<span style="color:#e6db74">&#39;actor&#39;</span>]<span style="color:#f92672">=</span> sel<span style="color:#f92672">.</span>xpath(<span style="color:#e6db74">&#39;//*[@id=&#34;info&#34;]/span[3]/a[1]/text()&#39;</span>)<span style="color:#f92672">.</span>extract()
       <span style="color:#66d9ef">return</span> item</code></pre></div></li>
</ol>

<blockquote>
<p>说明：上面我们通过Scrapy提供的爬虫模板创建了Spider，其中的rules中的LinkExtractor对象会自动完成对新的链接的解析，该对象中有一个名为extract_link的回调方法。Scrapy支持用XPath语法和CSS选择器进行数据解析，对应的方法分别是xpath和css，上面我们使用了XPath语法对页面进行解析，如果不熟悉XPath语法可以看看后面的补充说明。</p>
</blockquote>

<p>到这里，我们已经可以通过下面的命令让爬虫运转起来。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Shell" data-lang="Shell">   <span style="color:#f92672">(</span>venv<span style="color:#f92672">)</span>$ scrapy crawl movie</code></pre></div>
<p>可以在控制台看到爬取到的数据，如果想将这些数据保存到文件中，可以通过<code>-o</code>参数来指定文件名，Scrapy支持我们将爬取到的数据导出成JSON、CSV、XML、pickle、marshal等格式。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Shell" data-lang="Shell">   <span style="color:#f92672">(</span>venv<span style="color:#f92672">)</span>$ scrapy crawl moive -o result.json</code></pre></div>
<ol>
<li><p>在pipelines.py中完成对数据进行持久化的操作。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#75715e"># -*- coding: utf-8 -*-</span>

<span style="color:#75715e"># Define your item pipelines here</span>
<span style="color:#75715e">#</span>
<span style="color:#75715e"># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span style="color:#75715e"># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span>
<span style="color:#f92672">import</span> pymongo

<span style="color:#f92672">from</span> scrapy.exceptions <span style="color:#f92672">import</span> DropItem
<span style="color:#f92672">from</span> scrapy.conf <span style="color:#f92672">import</span> settings
<span style="color:#f92672">from</span> scrapy <span style="color:#f92672">import</span> log


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DoubanPipeline</span>(object):

   <span style="color:#66d9ef">def</span> __init__(self):
       connection <span style="color:#f92672">=</span> pymongo<span style="color:#f92672">.</span>MongoClient(settings[<span style="color:#e6db74">&#39;MONGODB_SERVER&#39;</span>], settings[<span style="color:#e6db74">&#39;MONGODB_PORT&#39;</span>])
       db <span style="color:#f92672">=</span> connection[settings[<span style="color:#e6db74">&#39;MONGODB_DB&#39;</span>]]
       self<span style="color:#f92672">.</span>collection <span style="color:#f92672">=</span> db[settings[<span style="color:#e6db74">&#39;MONGODB_COLLECTION&#39;</span>]]

   <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">process_item</span>(self, item, spider):
       <span style="color:#75715e">#Remove invalid data</span>
       valid <span style="color:#f92672">=</span> True
       <span style="color:#66d9ef">for</span> data <span style="color:#f92672">in</span> item:
         <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> data:
           valid <span style="color:#f92672">=</span> False
           <span style="color:#66d9ef">raise</span> DropItem(<span style="color:#e6db74">&#34;Missing </span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> of blogpost from </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span>(data, item[<span style="color:#e6db74">&#39;url&#39;</span>]))
       <span style="color:#66d9ef">if</span> valid:
       <span style="color:#75715e">#Insert data into database</span>
           new_moive<span style="color:#f92672">=</span>[{
               <span style="color:#e6db74">&#34;name&#34;</span>:item[<span style="color:#e6db74">&#39;name&#39;</span>][<span style="color:#ae81ff">0</span>],
               <span style="color:#e6db74">&#34;year&#34;</span>:item[<span style="color:#e6db74">&#39;year&#39;</span>][<span style="color:#ae81ff">0</span>],
               <span style="color:#e6db74">&#34;score&#34;</span>:item[<span style="color:#e6db74">&#39;score&#39;</span>],
               <span style="color:#e6db74">&#34;director&#34;</span>:item[<span style="color:#e6db74">&#39;director&#39;</span>],
               <span style="color:#e6db74">&#34;classification&#34;</span>:item[<span style="color:#e6db74">&#39;classification&#39;</span>],
               <span style="color:#e6db74">&#34;actor&#34;</span>:item[<span style="color:#e6db74">&#39;actor&#39;</span>]
           }]
           self<span style="color:#f92672">.</span>collection<span style="color:#f92672">.</span>insert(new_moive)
           log<span style="color:#f92672">.</span>msg(<span style="color:#e6db74">&#34;Item wrote to MongoDB database </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">/</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span>
           (settings[<span style="color:#e6db74">&#39;MONGODB_DB&#39;</span>], settings[<span style="color:#e6db74">&#39;MONGODB_COLLECTION&#39;</span>]),
           level<span style="color:#f92672">=</span>log<span style="color:#f92672">.</span>DEBUG, spider<span style="color:#f92672">=</span>spider)
       <span style="color:#66d9ef">return</span> item</code></pre></div></li>
</ol>

<p>利用Pipeline我们可以完成以下操作：</p>

<ul>
<li>清理HTML数据，验证爬取的数据。</li>
<li>丢弃重复的不必要的内容。</li>
<li>将爬取的结果进行持久化操作。</li>
</ul>

<ol>
<li><p>修改settings.py文件对项目进行配置。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-Python" data-lang="Python"><span style="color:#75715e"># -*- coding: utf-8 -*-</span>

<span style="color:#75715e"># Scrapy settings for douban project</span>
<span style="color:#75715e">#</span>
<span style="color:#75715e"># For simplicity, this file contains only settings considered important or</span>
<span style="color:#75715e"># commonly used. You can find more settings consulting the documentation:</span>
<span style="color:#75715e">#</span>
<span style="color:#75715e">#     https://doc.scrapy.org/en/latest/topics/settings.html</span>
<span style="color:#75715e">#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html</span>
<span style="color:#75715e">#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html</span>

BOT_NAME <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;douban&#39;</span>

SPIDER_MODULES <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;douban.spiders&#39;</span>]
NEWSPIDER_MODULE <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;douban.spiders&#39;</span>


<span style="color:#75715e"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span>
USER_AGENT <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.54 Safari/536.5&#39;</span>

<span style="color:#75715e"># Obey robots.txt rules</span>
ROBOTSTXT_OBEY <span style="color:#f92672">=</span> True

<span style="color:#75715e"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span>
<span style="color:#75715e"># CONCURRENT_REQUESTS = 32</span>

<span style="color:#75715e"># Configure a delay for requests for the same website (default: 0)</span>
<span style="color:#75715e"># See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay</span>
<span style="color:#75715e"># See also autothrottle settings and docs</span>
DOWNLOAD_DELAY <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
RANDOMIZE_DOWNLOAD_DELAY <span style="color:#f92672">=</span> True
<span style="color:#75715e"># The download delay setting will honor only one of:</span>
<span style="color:#75715e"># CONCURRENT_REQUESTS_PER_DOMAIN = 16</span>
<span style="color:#75715e"># CONCURRENT_REQUESTS_PER_IP = 16</span>

<span style="color:#75715e"># Disable cookies (enabled by default)</span>
COOKIES_ENABLED <span style="color:#f92672">=</span> True

MONGODB_SERVER <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;120.77.222.217&#39;</span>
MONGODB_PORT <span style="color:#f92672">=</span> <span style="color:#ae81ff">27017</span>
MONGODB_DB <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;douban&#39;</span>
MONGODB_COLLECTION <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;movie&#39;</span>

<span style="color:#75715e"># Disable Telnet Console (enabled by default)</span>
<span style="color:#75715e"># TELNETCONSOLE_ENABLED = False</span>

<span style="color:#75715e"># Override the default request headers:</span>
<span style="color:#75715e"># DEFAULT_REQUEST_HEADERS = {</span>
<span style="color:#75715e">#   &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,</span>
<span style="color:#75715e">#   &#39;Accept-Language&#39;: &#39;en&#39;,</span>
<span style="color:#75715e"># }</span>

<span style="color:#75715e"># Enable or disable spider middlewares</span>
<span style="color:#75715e"># See https://doc.scrapy.org/en/latest/topics/spider-middleware.html</span>
<span style="color:#75715e"># SPIDER_MIDDLEWARES = {</span>
<span style="color:#75715e">#    &#39;douban.middlewares.DoubanSpiderMiddleware&#39;: 543,</span>
<span style="color:#75715e"># }</span>

<span style="color:#75715e"># Enable or disable downloader middlewares</span>
<span style="color:#75715e"># See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html</span>
<span style="color:#75715e"># DOWNLOADER_MIDDLEWARES = {</span>
<span style="color:#75715e">#    &#39;douban.middlewares.DoubanDownloaderMiddleware&#39;: 543,</span>
<span style="color:#75715e"># }</span>

<span style="color:#75715e"># Enable or disable extensions</span>
<span style="color:#75715e"># See https://doc.scrapy.org/en/latest/topics/extensions.html</span>
<span style="color:#75715e"># EXTENSIONS = {</span>
<span style="color:#75715e">#    &#39;scrapy.extensions.telnet.TelnetConsole&#39;: None,</span>
<span style="color:#75715e"># }</span>

<span style="color:#75715e"># Configure item pipelines</span>
<span style="color:#75715e"># See https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span>
ITEM_PIPELINES <span style="color:#f92672">=</span> {
   <span style="color:#e6db74">&#39;douban.pipelines.DoubanPipeline&#39;</span>: <span style="color:#ae81ff">400</span>,
}

LOG_LEVEL <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;DEBUG&#39;</span>

<span style="color:#75715e"># Enable and configure the AutoThrottle extension (disabled by default)</span>
<span style="color:#75715e"># See https://doc.scrapy.org/en/latest/topics/autothrottle.html</span>
<span style="color:#75715e">#AUTOTHROTTLE_ENABLED = True</span>
<span style="color:#75715e"># The initial download delay</span>
<span style="color:#75715e">#AUTOTHROTTLE_START_DELAY = 5</span>
<span style="color:#75715e"># The maximum download delay to be set in case of high latencies</span>
<span style="color:#75715e">#AUTOTHROTTLE_MAX_DELAY = 60</span>
<span style="color:#75715e"># The average number of requests Scrapy should be sending in parallel to</span>
<span style="color:#75715e"># each remote server</span>
<span style="color:#75715e">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span>
<span style="color:#75715e"># Enable showing throttling stats for every response received:</span>
<span style="color:#75715e">#AUTOTHROTTLE_DEBUG = False</span>

<span style="color:#75715e"># Enable and configure HTTP caching (disabled by default)</span>
<span style="color:#75715e"># See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span>
HTTPCACHE_ENABLED <span style="color:#f92672">=</span> True
HTTPCACHE_EXPIRATION_SECS <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
HTTPCACHE_DIR <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;httpcache&#39;</span>
HTTPCACHE_IGNORE_HTTP_CODES <span style="color:#f92672">=</span> []
HTTPCACHE_STORAGE <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;scrapy.extensions.httpcache.FilesystemCacheStorage&#39;</span></code></pre></div></li>
</ol>
</article>

      

      
    </div>

    
  

  <aside class="book-toc level-3 fixed">
    <nav id="TableOfContents">
<ul>
<li><a href="#scrapy爬虫框架入门">Scrapy爬虫框架入门</a>
<ul>
<li><a href="#scrapy概述">Scrapy概述</a>
<ul>
<li><a href="#组件">组件</a></li>
<li><a href="#数据处理流程">数据处理流程</a></li>
</ul></li>
<li><a href="#安装和使用scrapy">安装和使用Scrapy</a></li>
</ul></li>
</ul>
</nav>
  </aside>



  </main>

  
  
</body>

</html>
